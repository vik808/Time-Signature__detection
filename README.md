ğŸ“Œ Abstract

This project explores the automatic detection of musical time signatures from raw audio recordings. Framed as a 4-class classification problem (3/4, 4/4, 5/4, 7/4), it leverages deep learning techniques to extract rhythmic patterns using mel spectrograms and classify them using models such as Custom CNNs, ResNet-18, and EfficientNet-B0. We work with a curated dataset of 2800 audio samples, applying robust preprocessing, augmentation, and training strategies to achieve high classification accuracy. This pipeline contributes significantly to music information retrieval, AI-assisted music tools, and educational software. ğŸ“– Table of Contents

Introduction

Background

Motivation

Objectives

Dataset Description

Feature Engineering

Data Preprocessing

Models

Tools and Libraries

Training Process

Results

Evaluation Metrics

Performance Analysis

Limitations

Conclusion

Key Findings

Real-World Applications

Future Work

Project Structure
ğŸ“˜ Introduction

Time signature, or meter, is the rhythmic foundation of music. Automatically identifying it from raw audio enables intelligent music processing, understanding, and composition. This project builds a pipeline to detect time signatures directly from WAV files, using modern deep learning methods. ğŸ§  Background

Traditional music analysis focuses on symbolic representations like MIDI. However, real-world applications often involve raw, noisy audio. In this project, we use deep learning on mel spectrograms to approximate temporal rhythmic structure and identify meter without relying on symbolic data. ğŸ’¡ Motivation

Most music AI systems skip over time signature analysis or depend on symbolic input. We aim to:

Detect rhythmic structure directly from raw audio

Assist educators, composers, and researchers

Improve MIR tools with accurate time signature tagging
ğŸ¯ Objectives

Build a robust classifier for time signatures (3/4, 4/4, 5/4, 7/4)

Apply data augmentation to increase robustness

Evaluate different deep learning models

Visualize and analyze classification results
ğŸ“Š Meter2800 Dataset Overview ğŸ“ General Information:

Total audio tracks: 2800

Total size: ~2.26 GB

Genres included: Rock, Pop, Classical, Jazz

Annotations available:

    Tempo

    Meter class (time signature)
ğŸ—ƒï¸ Data Sources Breakdown (Table 1): Data Source Total Annotated Train Test

Data Source	Total Annotated	Train	Test
FMA	851	598	253
GTZAN	911	632	279
MAG	925	652	273
OWN	113	78	35
Total	2800	1960	840
ğŸ•’ Meter Class Distribution (Table 2): Meter Class Number of Files

Meter Class	Number of Files
3/4	1200
4/4	1200
5/4	200
7/4	200
The dataset is imbalanced, with 3/4 and 4/4 dominating the class distribution.

ğŸ¼ Feature Engineering

Mel Spectrograms extracted using librosa

128 mel bands with time-frequency structure

Fixed-length windows for consistent input
ğŸ§¹ Data Preprocessing

Unzip and convert MP3s to WAV

Apply augmentation (pitch, tempo)

Extract mel spectrograms

Save data to CSV with labels

Visualize sample spectrograms
ğŸ§  Models âœ… Custom CNN

Simple 2D CNN with BatchNorm + ReLU

Lightweight and easy to train
âœ… ResNet-18

Pretrained on ImageNet

Finetuned for 4-class classification
âœ… EfficientNet-B0 â­

Most accurate model

High efficiency with low parameter count

Best performance on test set
ğŸ› ï¸ Tools and Libraries

Python, NumPy, Pandas

Librosa, Pydub, Audiomentations

PyTorch, Torchvision

Matplotlib, Seaborn

Google Colab (T4 GPU + AMP)
ğŸ§ª Training Process

Optimizer: Adam

Loss: CrossEntropyLoss

Epochs: ~25

Batch Size: 64

Mixed Precision (AMP): Enabled

Train/Validation/Test Split: Stratified
ğŸ“Š Results Model Accuracy F1 Score Notes CNN (Custom) ~75% Moderate Good for baseline ResNet-18 ~82% Strong Transfer learning works well EfficientNet ~87% Best Most accurate and efficient ğŸ“‰ Evaluation Metrics

Accuracy

Precision / Recall / F1-Score

Class-wise Performance

Confusion Matrix (per epoch)

Loss & Accuracy Curves
ğŸ“ˆ Performance Analysis

EfficientNet outperformed all models.

3/4 and 4/4 sometimes confused due to similar rhythm.

Augmentation greatly improved generalization.

Minor class imbalance affected 5/4 and 7/4 accuracy.
âš ï¸ Limitations

Dataset size is moderate for deep learning.

Ambiguity in rhythm for closely spaced classes.

Augmentation cannot fully mimic real-world complexity.
âœ… Conclusion

We successfully built a deep learning pipeline to classify time signatures from raw audio. EfficientNet-B0 emerged as the best model. This work shows that rhythm classification from raw signals is not only feasible but can be applied to real-world music systems. ğŸ”‘ Key Findings

Mel spectrograms are suitable features for meter detection.

EfficientNet achieves high accuracy with fewer resources.

Augmentation techniques improve model generalization.
ğŸŒ Real-World Applications

ğŸ“ Music Education: Auto-evaluation tools for rhythm training

ğŸ§  MIR Systems: Improved search and tagging by meter

ğŸ¼ Composition Tools: AI-assisted music generators with rhythm awareness

ğŸ” Musicology: Quantitative rhythm analysis of large corpora
ğŸ”­ Future Work

Add compound and irregular time signatures (e.g., 6/8, 9/8, 11/8)

Explore beat tracking + spectrogram fusion

Experiment with transformer-based audio models

Deploy as a web app or VST plugin
About
No description, website, or topics provided.
Resources
 Readme
 Activity
Stars
 0 stars
Watchers
 0 watching
Forks
 0 forks
Report repository
Releases
No releases published
Packages
No packages published
Languages
Jupyter Notebook
100.0%
Footer
